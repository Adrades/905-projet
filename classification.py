# -*- coding: utf-8 -*-
"""Copie de classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W218Wx2JzKtBdu-a9tDfVJysCS8tfQMr

Repris en grande partie de https://www.tensorflow.org/tutorials/images/classification
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf
import pathlib

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
"""TODO LINKS:


https://www.tensorflow.org/guide/keras/custom_layers_and_models

https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory

https://www.tensorflow.org/api_docs/python/tf/data/Dataset

https://www.tensorflow.org/tutorials/images/classification

https://jonathan-hui.medium.com/tensorflow-dataset-data-preparation-b81fcf9c3c44


"""
"""
# Connection à mon dossier google drive pour la récupération des images
from google.colab import drive
drive.mount('/content/drive')

data_dir = "/content/drive/My Drive/Colab Notebooks/garbage"
"""
data_dir = "src/garbage"

data_dir = pathlib.Path(data_dir)
image_count = len(list(data_dir.glob('*/*.jpg')))
print(f"Image Count: {image_count}")

# chargement avec la bibliothèque PIL
absolute_path = os.path.abspath('.')
img = PIL.Image.open(str(next(data_dir.glob('metal/*'))))
img_height, img_width = img.size

# Séparation des images en 2 groupes, aléatoires avec seed fix
batch_size = 32
val_split = 0.1
seed = 707

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=val_split,
  subset="training",
  seed=seed,
  image_size=(img_height, img_width),
  batch_size=batch_size
)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=val_split,
  subset="validation",
  seed=seed,
  image_size=(img_height, img_width),
  batch_size=batch_size
)

class_names = train_ds.class_names
num_classes = len(class_names)

# Rescaling for memory space
train_ds = train_ds.map(lambda x, y: (layers.Rescaling(1./255)(x), y))
val_ds = val_ds.map(lambda x, y: (layers.Rescaling(1./255)(x), y))

train_ds = train_ds.map(lambda x, y: (layers.Resizing(128, 128, interpolation='bilinear')(x), y))
val_ds = val_ds.map(lambda x, y: (layers.Resizing(128, 128, interpolation='bilinear')(x), y))


# Augmentation des datatsets

aug_model = Sequential(
    [
        layers.RandomFlip("horizontal_and_vertical", input_shape=(128, 128, 3)),
        layers.RandomRotation(factor=0.15),
        # layers.RandomContrast(factor=0.1),
    ],
    name="augmentation_model",
)

# Pour le gpu
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""Afin de rendre mon code plus propre et compréhensible, j'ai essayé de respecter la PEP8. J'ai préféré l'utilisation des classes tels que dans l'exemple de l'autoencodeur, et j'ai imiter cette façon de faire pour la classsification simple.

### Tests effectués avec la classification

Autres Optimizers que Adam, aucune améliorations.

(Conv2D + MaxPooling2D) * 3 + Flatten + Dense128 + Dense6
val_acc: 0.6491

(Conv2D + MaxPooling2D) * 2 + Flatten + Dense128 + Dense6
val_acc: 0.469


(Conv2D + MaxPooling2D) * 1 + Flatten + Dense128 + Dense6
val_acc: 0.5921


Après quelques relances, j'ai constaté que les performances étaient très aléatoires, allant de 0.47 à 0.70 pour le même modèle sans changement. J'ai donc essayé d'avoir des performances plus stables, sans succès.

Je pense qu'un plus grand nombre d'epoch pour le fit permettrai de retrouver de meilleures performances, mais il y a une bonne chance de faire de l'overfitting.


"""

# Creating model

class Classification(Model):
  def __init__(self):
    super(Classification, self).__init__()
    self.classifier = Sequential([
      aug_model,
      layers.Conv2D(16, 3, padding='same', activation='relu'),
      layers.MaxPooling2D(),
      layers.Conv2D(32, 3, padding='same', activation='relu'),
      layers.AveragePooling2D(),
      layers.Conv2D(64, 3, padding='same', activation='relu'),
      layers.AveragePooling2D(),
      layers.Flatten(),
      layers.Dense(512, activation='relu'),
      layers.Dense(128, activation='relu'),
      layers.Dense(num_classes)
      ],
      name="Classifier")

  def call(self, x):
    return self.classifier(x)


classie = Classification()
classie.compile(
    optimizer='adam', # Best opti with other adam-derived optimizers
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Pretty good, adapted
    metrics=['accuracy']
)
# classie.classifier.summary()

epochs=15
history = classie.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,
  verbose=0,
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

"""
plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
"""

print(
    f"Simple Classificater accuracy of {val_acc[-1]:.4f}"
)

"""## Partie Décodeur Encodeur

Pour cette partie, j'ai repris le code d'exemple en lien sur moodle.

### Tests effectués sur l'encodage décodage

1.
Encodage Flaten + Dense
Decodage Dense + Reshape
val_acc: .7528

2.
Encodage Conv2D + Flaten + Dense
Decodage Dense + Reshape
val_acc: 0.6765
But better classification afterward

"""

class Encoder(Model):
  def __init__(self,):
    super(Encoder, self).__init__()
    self.latent_dim = 512
    # Très bon rapport train_acc val_acc mais monte difficilement >0.70
    self.encoder = tf.keras.Sequential([
      layers.Conv2D(16, 3, padding='same', activation='relu'),
      layers.MaxPooling2D(),
      layers.Conv2D(32, 3, padding='same', activation='relu'),
      layers.AveragePooling2D(),
      layers.Flatten(),
      layers.Dense(self.latent_dim, activation='relu'),
      ],
      name="Encoder",
    )
    self.decoder = tf.keras.Sequential([
      layers.Dense(128*128*3, activation='relu'),
      layers.Reshape((128, 128, 3))
      ],
      name="Decoder",
    )

  def call(self, x):
    augmented = aug_model(x)
    encoded = self.encoder(augmented)
    decoded = self.decoder(encoded)
    return decoded

encoder = Encoder()
encoder.compile(
    optimizer='adam',
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=['accuracy'],
)
# encoder.encoder.summary()
# encoder.decoder.summary()

# Cast dataset to numpy array for fit
x_train = np.concatenate([x for x, y in train_ds], axis=0)
x_test = np.concatenate([x for x, y in val_ds], axis=0)

history = encoder.fit(
  x=x_train, y=x_train,
  validation_data=(x_test, x_test),
  epochs=10,
  verbose=1,
)

val_acc = history.history['val_accuracy']
print(
    f"The encoder-decoder accuracy of {val_acc[-1]:.4f}"
)

"""## Tests effectués sur la classification post encodage

Avec 0.7528 ED, dense only:
  - Dense128 + Dense6
  - val_acc: 0.443

Avec 0.6765 ED, conv2D + dense:
  - Dense128 + Dense128 + Dense6
  - val_acc: 0.535

"""

class EncodedClassifier(Model):
  def __init__(self, encoder_layer):
    super(EncodedClassifier, self).__init__()
    # No copy for ram save
    self.encoder = encoder_layer
    self.encoder.trainable = False
    self.classifier = tf.keras.Sequential([
      self.encoder,
      layers.Dense(512, activation='relu'),
      layers.Dense(512, activation='relu'),
      layers.Dense(num_classes)
      ],
      name="EncodedClassifier",
    )

  def call(self, x):
    return self.classifier(x)

eclassie = EncodedClassifier(encoder.encoder)
eclassie.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'],
)

history = eclassie.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,
  verbose=0,
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

print(
    f"Encoder-Classificater accuracy of {val_acc[-1]:.4f}"
)
